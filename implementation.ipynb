{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgKA7pUT8cngU5wyvBy8xV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arun-mac/text-classification-using-transformers/blob/main/implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the necessary libraries"
      ],
      "metadata": {
        "id": "j53twWBOvUen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLbFqtsSkYv9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Transformer blocks and positional embedding"
      ],
      "metadata": {
        "id": "yj2jxKp5vvxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines two custom layers for building a transformer-based neural network:\n",
        "\n",
        "**Transformer Block:**\n",
        "\n",
        "\n",
        "1.   This layer implements a single block of the transformer architecture.\n",
        "2.   It consists of a multi-head self-attention mechanism (MultiHeadAttention) followed by a feedforward neural network (Sequential of dense layers with ReLU activation).\n",
        "3. Layer normalization and dropout are applied after each sub-layer.\n",
        "4. The output is the sum of the input and the result of the feedforward neural network after normalization and dropout.\n",
        "\n",
        "**Token and Position Embedding:**\n",
        "\n",
        "\n",
        "1.   This layer combines token embeddings and positional embeddings.\n",
        "2.   It uses two separate embedding layers (Embedding): one for token embeddings based on the vocabulary size, and the other for positional embeddings based on the maximum sequence length.\n",
        "3. The positional embeddings are added to the token embeddings.\n",
        "4. The output is the combination of token and positional embeddings.\n"
      ],
      "metadata": {
        "id": "XPwKOQzFwFty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "vP6A7HuFkhhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "_ERoFCKikk6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ],
      "metadata": {
        "id": "x23GKip4xK_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the IMDB movie reviews dataset, limiting the vocabulary to the top 20,000 words and sequences to a maximum length of 400. It splits the data into training and validation sets, printing the respective sequence counts."
      ],
      "metadata": {
        "id": "ex_4YDRnxNTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "maxlen = 400\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpIkkmc_kqjo",
        "outputId": "1f606c83-3ab5-45ce-bea7-6972fce623db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsFZP1ovkv7p",
        "outputId": "d7f0a762-2a34-4aeb-db67-b77db7864644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "VmeVP8Gpk5jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition and training"
      ],
      "metadata": {
        "id": "runQ5TWfxnk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines a transformer-based neural network for sequence classification using Keras. The model includes an embedding layer, a transformer block, global average pooling, dropout for regularization, and dense layers with ReLU activation. The final output layer uses softmax activation for binary classification. The model is designed for tasks like sentiment analysis on sequences with a maximum length of 400 words and a vocabulary size of 20,000."
      ],
      "metadata": {
        "id": "UEd7j22qx3tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "shwrj1FOk8M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=64, epochs=2,\n",
        "                    validation_data=(x_val, y_val)\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzIqYuwwk_DK",
        "outputId": "ad7d2c8f-c6cf-4e46-e091-0fe08feddaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 75s 167ms/step - loss: 0.4799 - accuracy: 0.7330 - val_loss: 0.2794 - val_accuracy: 0.8847\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 41s 105ms/step - loss: 0.2055 - accuracy: 0.9229 - val_loss: 0.2697 - val_accuracy: 0.8901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"predict_class.h5\")"
      ],
      "metadata": {
        "id": "gGvrdPN-lDPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "86VQo1UmyWQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code evaluates the previously defined transformer-based model on the validation data and prints the results, including metrics such as accuracy and loss."
      ],
      "metadata": {
        "id": "fqK7ahQYya1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_val, y_val, verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkLphldblGYS",
        "outputId": "87bc8525-0a02-4ade-f704-1d786231aee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 - 5s - loss: 0.2697 - accuracy: 0.8901 - 5s/epoch - 6ms/step\n",
            "loss: 0.270\n",
            "accuracy: 0.890\n"
          ]
        }
      ]
    }
  ]
}